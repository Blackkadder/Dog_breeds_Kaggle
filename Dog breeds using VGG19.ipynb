{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import keras\n",
    "from keras.applications.vgg19 import VGG19\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "\n",
    "import os\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from tqdm import tqdm # helpful for loops\n",
    "import cv2 # OpenCV\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('labels.csv')\n",
    "df_test = pd.read_csv('sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>breed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10217</th>\n",
       "      <td>ffd25009d635cfd16e793503ac5edef0</td>\n",
       "      <td>borzoi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10218</th>\n",
       "      <td>ffd3f636f7f379c51ba3648a9ff8254f</td>\n",
       "      <td>dandie_dinmont</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10219</th>\n",
       "      <td>ffe2ca6c940cddfee68fa3cc6c63213f</td>\n",
       "      <td>airedale</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10220</th>\n",
       "      <td>ffe5f6d8e2bff356e9482a80a6e29aac</td>\n",
       "      <td>miniature_pinscher</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10221</th>\n",
       "      <td>fff43b07992508bc822f33d8ffd902ae</td>\n",
       "      <td>chesapeake_bay_retriever</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     id                     breed\n",
       "10217  ffd25009d635cfd16e793503ac5edef0                    borzoi\n",
       "10218  ffd3f636f7f379c51ba3648a9ff8254f            dandie_dinmont\n",
       "10219  ffe2ca6c940cddfee68fa3cc6c63213f                  airedale\n",
       "10220  ffe5f6d8e2bff356e9482a80a6e29aac        miniature_pinscher\n",
       "10221  fff43b07992508bc822f33d8ffd902ae  chesapeake_bay_retriever"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "targets_series = pd.Series(df_train['breed'])\n",
    "one_hot = pd.get_dummies(targets_series, sparse = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "one_hot_labels = np.asarray(one_hot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resize to 90x90"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "im_size = 90"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train = []\n",
    "y_train = []\n",
    "x_test = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create one hot encoded array of breed names for training set and resize images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 10222/10222 [01:37<00:00, 104.81it/s]\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "for f, breed in tqdm(df_train.values):\n",
    "    img = cv2.imread('train/{}.jpg'.format(f))\n",
    "    label = one_hot_labels[i]\n",
    "    x_train.append(cv2.resize(img, (im_size, im_size)))\n",
    "    y_train.append(label)\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do same for test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 10357/10357 [01:50<00:00, 93.69it/s]\n"
     ]
    }
   ],
   "source": [
    "for f in tqdm(df_test['id'].values):\n",
    "    img = cv2.imread('test/{}.jpg'.format(f))\n",
    "    x_test.append(cv2.resize(img, (im_size, im_size)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_train_raw = np.array(y_train, np.uint8)\n",
    "x_train_raw = np.array(x_train, np.float32) / 255.\n",
    "x_test  = np.array(x_test, np.float32) / 255."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10222, 90, 90, 3)\n",
      "(10222, 120)\n",
      "(10357, 90, 90, 3)\n"
     ]
    }
   ],
   "source": [
    "print(x_train_raw.shape)\n",
    "print(y_train_raw.shape)\n",
    "print(x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "120"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_class = y_train_raw.shape[1]\n",
    "num_class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "120 dog breeds in total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create test and training sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_valid, Y_train, Y_valid = train_test_split(x_train_raw, y_train_raw, \n",
    "                                    test_size=0.3, random_state=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using vgg19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 90, 90, 3)         0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 90, 90, 64)        1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 90, 90, 64)        36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 45, 45, 64)        0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 45, 45, 128)       73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 45, 45, 128)       147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 22, 22, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 22, 22, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 22, 22, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 22, 22, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv4 (Conv2D)        (None, 22, 22, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 11, 11, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 11, 11, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 11, 11, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 11, 11, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv4 (Conv2D)        (None, 11, 11, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 5, 5, 512)         0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 5, 5, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 5, 5, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 5, 5, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv4 (Conv2D)        (None, 5, 5, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 2, 2, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 120)               245880    \n",
      "=================================================================\n",
      "Total params: 20,270,264\n",
      "Trainable params: 245,880\n",
      "Non-trainable params: 20,024,384\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Create the base pre-trained model\n",
    "# Can't download weights in the kernel\n",
    "base_model = VGG19(#weights='imagenet',\n",
    "    weights = None, include_top=False, input_shape=(im_size, im_size, 3))\n",
    "\n",
    "# Add a new top layer\n",
    "x = base_model.output\n",
    "x = Flatten()(x)\n",
    "predictions = Dense(num_class, activation='softmax')(x)\n",
    "\n",
    "# This is the model we will train\n",
    "model = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "# First: train only the top layers (which were randomly initialized)\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', \n",
    "              optimizer='SGD', \n",
    "              metrics=['accuracy'])\n",
    "\n",
    "callbacks_list = [keras.callbacks.EarlyStopping(monitor='val_acc', patience=3, verbose=1)]\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7155 samples, validate on 3067 samples\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4240/7155 [================>.............] - ETA: 1:23:51 - loss: 4.7877 - acc: 0.0000e+ - ETA: 28:11 - loss: 4.7876 - acc: 0.0000e+00 - ETA: 17:00 - loss: 4.7876 - acc: 0.0000e+ - ETA: 12:12 - loss: 4.7875 - acc: 0.0000e+ - ETA: 9:33 - loss: 4.7874 - acc: 0.0000e+00 - ETA: 7:51 - loss: 4.7875 - acc: 0.0000e+0 - ETA: 6:40 - loss: 4.7875 - acc: 0.0000e+0 - ETA: 5:49 - loss: 4.7874 - acc: 0.0067    - ETA: 5:10 - loss: 4.7875 - acc: 0.005 - ETA: 4:38 - loss: 4.7874 - acc: 0.010 - ETA: 4:13 - loss: 4.7874 - acc: 0.009 - ETA: 3:53 - loss: 4.7874 - acc: 0.008 - ETA: 3:35 - loss: 4.7875 - acc: 0.008 - ETA: 3:20 - loss: 4.7875 - acc: 0.007 - ETA: 3:07 - loss: 4.7875 - acc: 0.006 - ETA: 2:56 - loss: 4.7875 - acc: 0.006 - ETA: 2:46 - loss: 4.7875 - acc: 0.006 - ETA: 2:38 - loss: 4.7875 - acc: 0.005 - ETA: 2:30 - loss: 4.7875 - acc: 0.005 - ETA: 2:23 - loss: 4.7876 - acc: 0.005 - ETA: 2:17 - loss: 4.7875 - acc: 0.004 - ETA: 2:11 - loss: 4.7876 - acc: 0.004 - ETA: 2:06 - loss: 4.7876 - acc: 0.004 - ETA: 2:01 - loss: 4.7876 - acc: 0.004 - ETA: 1:56 - loss: 4.7876 - acc: 0.004 - ETA: 1:50 - loss: 4.7875 - acc: 0.005 - ETA: 1:47 - loss: 4.7876 - acc: 0.005 - ETA: 1:43 - loss: 4.7876 - acc: 0.005 - ETA: 1:40 - loss: 4.7876 - acc: 0.005 - ETA: 1:36 - loss: 4.7876 - acc: 0.004 - ETA: 1:33 - loss: 4.7876 - acc: 0.004 - ETA: 1:31 - loss: 4.7876 - acc: 0.004 - ETA: 1:28 - loss: 4.7876 - acc: 0.004 - ETA: 1:26 - loss: 4.7876 - acc: 0.004 - ETA: 1:24 - loss: 4.7876 - acc: 0.004 - ETA: 1:22 - loss: 4.7876 - acc: 0.005 - ETA: 1:20 - loss: 4.7876 - acc: 0.005 - ETA: 1:18 - loss: 4.7876 - acc: 0.005 - ETA: 1:16 - loss: 4.7876 - acc: 0.005 - ETA: 1:14 - loss: 4.7876 - acc: 0.006 - ETA: 1:12 - loss: 4.7876 - acc: 0.006 - ETA: 1:11 - loss: 4.7876 - acc: 0.005 - ETA: 1:10 - loss: 4.7876 - acc: 0.005 - ETA: 1:08 - loss: 4.7876 - acc: 0.005 - ETA: 1:07 - loss: 4.7876 - acc: 0.005 - ETA: 1:06 - loss: 4.7876 - acc: 0.005 - ETA: 1:04 - loss: 4.7876 - acc: 0.005 - ETA: 1:03 - loss: 4.7876 - acc: 0.005 - ETA: 1:02 - loss: 4.7876 - acc: 0.006 - ETA: 1:01 - loss: 4.7876 - acc: 0.005 - ETA: 1:00 - loss: 4.7876 - acc: 0.005 - ETA: 59s - loss: 4.7876 - acc: 0.005 - ETA: 58s - loss: 4.7876 - acc: 0.00 - ETA: 57s - loss: 4.7876 - acc: 0.00 - ETA: 56s - loss: 4.7876 - acc: 0.00 - ETA: 56s - loss: 4.7876 - acc: 0.00 - ETA: 55s - loss: 4.7876 - acc: 0.00 - ETA: 54s - loss: 4.7876 - acc: 0.00 - ETA: 53s - loss: 4.7876 - acc: 0.00 - ETA: 52s - loss: 4.7876 - acc: 0.00 - ETA: 52s - loss: 4.7876 - acc: 0.00 - ETA: 51s - loss: 4.7876 - acc: 0.00 - ETA: 50s - loss: 4.7876 - acc: 0.00 - ETA: 50s - loss: 4.7876 - acc: 0.00 - ETA: 49s - loss: 4.7876 - acc: 0.00 - ETA: 48s - loss: 4.7876 - acc: 0.00 - ETA: 48s - loss: 4.7876 - acc: 0.00 - ETA: 47s - loss: 4.7875 - acc: 0.00 - ETA: 46s - loss: 4.7875 - acc: 0.00 - ETA: 46s - loss: 4.7876 - acc: 0.00 - ETA: 45s - loss: 4.7876 - acc: 0.00 - ETA: 45s - loss: 4.7876 - acc: 0.00 - ETA: 44s - loss: 4.7876 - acc: 0.00 - ETA: 44s - loss: 4.7876 - acc: 0.00 - ETA: 43s - loss: 4.7875 - acc: 0.00 - ETA: 43s - loss: 4.7875 - acc: 0.00 - ETA: 42s - loss: 4.7875 - acc: 0.00 - ETA: 42s - loss: 4.7875 - acc: 0.00 - ETA: 41s - loss: 4.7875 - acc: 0.00 - ETA: 41s - loss: 4.7875 - acc: 0.00 - ETA: 40s - loss: 4.7875 - acc: 0.00 - ETA: 40s - loss: 4.7875 - acc: 0.00 - ETA: 40s - loss: 4.7875 - acc: 0.00 - ETA: 39s - loss: 4.7875 - acc: 0.00 - ETA: 39s - loss: 4.7875 - acc: 0.00 - ETA: 38s - loss: 4.7875 - acc: 0.00 - ETA: 38s - loss: 4.7875 - acc: 0.00 - ETA: 38s - loss: 4.7875 - acc: 0.00 - ETA: 37s - loss: 4.7875 - acc: 0.00 - ETA: 37s - loss: 4.7874 - acc: 0.00 - ETA: 36s - loss: 4.7874 - acc: 0.00 - ETA: 36s - loss: 4.7874 - acc: 0.00 - ETA: 36s - loss: 4.7874 - acc: 0.00 - ETA: 35s - loss: 4.7875 - acc: 0.00 - ETA: 35s - loss: 4.7874 - acc: 0.00 - ETA: 35s - loss: 4.7875 - acc: 0.00 - ETA: 34s - loss: 4.7874 - acc: 0.00 - ETA: 34s - loss: 4.7875 - acc: 0.00 - ETA: 34s - loss: 4.7874 - acc: 0.00 - ETA: 34s - loss: 4.7874 - acc: 0.00 - ETA: 33s - loss: 4.7874 - acc: 0.00 - ETA: 33s - loss: 4.7874 - acc: 0.00 - ETA: 33s - loss: 4.7874 - acc: 0.00 - ETA: 32s - loss: 4.7874 - acc: 0.00 - ETA: 32s - loss: 4.7874 - acc: 0.00 - ETA: 32s - loss: 4.7874 - acc: 0.00 - ETA: 31s - loss: 4.7874 - acc: 0.00 - ETA: 31s - loss: 4.7874 - acc: 0.00 - ETA: 31s - loss: 4.7874 - acc: 0.00 - ETA: 31s - loss: 4.7874 - acc: 0.00 - ETA: 30s - loss: 4.7874 - acc: 0.00 - ETA: 30s - loss: 4.7874 - acc: 0.00 - ETA: 30s - loss: 4.7874 - acc: 0.00 - ETA: 30s - loss: 4.7874 - acc: 0.00 - ETA: 29s - loss: 4.7874 - acc: 0.00 - ETA: 29s - loss: 4.7874 - acc: 0.00 - ETA: 29s - loss: 4.7874 - acc: 0.00 - ETA: 29s - loss: 4.7874 - acc: 0.00 - ETA: 29s - loss: 4.7874 - acc: 0.00 - ETA: 28s - loss: 4.7874 - acc: 0.00 - ETA: 28s - loss: 4.7874 - acc: 0.00 - ETA: 28s - loss: 4.7874 - acc: 0.00 - ETA: 28s - loss: 4.7874 - acc: 0.00 - ETA: 27s - loss: 4.7874 - acc: 0.00 - ETA: 27s - loss: 4.7874 - acc: 0.00 - ETA: 27s - loss: 4.7874 - acc: 0.00 - ETA: 27s - loss: 4.7874 - acc: 0.00 - ETA: 26s - loss: 4.7874 - acc: 0.00 - ETA: 26s - loss: 4.7874 - acc: 0.00 - ETA: 26s - loss: 4.7874 - acc: 0.00 - ETA: 26s - loss: 4.7874 - acc: 0.00 - ETA: 25s - loss: 4.7874 - acc: 0.00 - ETA: 25s - loss: 4.7874 - acc: 0.00 - ETA: 25s - loss: 4.7874 - acc: 0.00 - ETA: 25s - loss: 4.7874 - acc: 0.00 - ETA: 25s - loss: 4.7874 - acc: 0.00 - ETA: 24s - loss: 4.7874 - acc: 0.00 - ETA: 24s - loss: 4.7874 - acc: 0.00 - ETA: 24s - loss: 4.7874 - acc: 0.00 - ETA: 24s - loss: 4.7874 - acc: 0.00 - ETA: 24s - loss: 4.7874 - acc: 0.00 - ETA: 24s - loss: 4.7874 - acc: 0.00 - ETA: 23s - loss: 4.7874 - acc: 0.00 - ETA: 23s - loss: 4.7874 - acc: 0.00 - ETA: 23s - loss: 4.7874 - acc: 0.00 - ETA: 23s - loss: 4.7874 - acc: 0.00 - ETA: 23s - loss: 4.7874 - acc: 0.00 - ETA: 22s - loss: 4.7874 - acc: 0.00 - ETA: 22s - loss: 4.7874 - acc: 0.00 - ETA: 22s - loss: 4.7874 - acc: 0.00 - ETA: 22s - loss: 4.7874 - acc: 0.00 - ETA: 22s - loss: 4.7874 - acc: 0.00 - ETA: 21s - loss: 4.7874 - acc: 0.00 - ETA: 21s - loss: 4.7874 - acc: 0.00 - ETA: 21s - loss: 4.7874 - acc: 0.00 - ETA: 21s - loss: 4.7874 - acc: 0.00 - ETA: 21s - loss: 4.7874 - acc: 0.00 - ETA: 21s - loss: 4.7874 - acc: 0.00 - ETA: 21s - loss: 4.7874 - acc: 0.00 - ETA: 20s - loss: 4.7874 - acc: 0.00 - ETA: 20s - loss: 4.7874 - acc: 0.00 - ETA: 20s - loss: 4.7874 - acc: 0.00 - ETA: 20s - loss: 4.7874 - acc: 0.00 - ETA: 20s - loss: 4.7874 - acc: 0.00 - ETA: 20s - loss: 4.7874 - acc: 0.00 - ETA: 19s - loss: 4.7874 - acc: 0.00 - ETA: 19s - loss: 4.7874 - acc: 0.00 - ETA: 19s - loss: 4.7873 - acc: 0.00 - ETA: 19s - loss: 4.7874 - acc: 0.00 - ETA: 19s - loss: 4.7874 - acc: 0.00 - ETA: 19s - loss: 4.7874 - acc: 0.00 - ETA: 19s - loss: 4.7873 - acc: 0.00 - ETA: 18s - loss: 4.7873 - acc: 0.00 - ETA: 18s - loss: 4.7873 - acc: 0.00 - ETA: 18s - loss: 4.7873 - acc: 0.00 - ETA: 18s - loss: 4.7873 - acc: 0.00 - ETA: 18s - loss: 4.7873 - acc: 0.00 - ETA: 18s - loss: 4.7873 - acc: 0.00 - ETA: 18s - loss: 4.7873 - acc: 0.00 - ETA: 17s - loss: 4.7873 - acc: 0.00 - ETA: 17s - loss: 4.7873 - acc: 0.00 - ETA: 17s - loss: 4.7873 - acc: 0.00 - ETA: 17s - loss: 4.7873 - acc: 0.00 - ETA: 17s - loss: 4.7873 - acc: 0.00 - ETA: 17s - loss: 4.7873 - acc: 0.00 - ETA: 17s - loss: 4.7873 - acc: 0.00 - ETA: 16s - loss: 4.7873 - acc: 0.00 - ETA: 16s - loss: 4.7873 - acc: 0.00 - ETA: 16s - loss: 4.7873 - acc: 0.00 - ETA: 16s - loss: 4.7872 - acc: 0.00 - ETA: 16s - loss: 4.7872 - acc: 0.00 - ETA: 16s - loss: 4.7872 - acc: 0.00 - ETA: 16s - loss: 4.7872 - acc: 0.00 - ETA: 16s - loss: 4.7872 - acc: 0.00 - ETA: 15s - loss: 4.7872 - acc: 0.00 - ETA: 15s - loss: 4.7872 - acc: 0.00 - ETA: 15s - loss: 4.7872 - acc: 0.00 - ETA: 15s - loss: 4.7872 - acc: 0.00 - ETA: 15s - loss: 4.7872 - acc: 0.00 - ETA: 15s - loss: 4.7872 - acc: 0.00 - ETA: 15s - loss: 4.7872 - acc: 0.00 - ETA: 15s - loss: 4.7872 - acc: 0.00 - ETA: 14s - loss: 4.7872 - acc: 0.00 - ETA: 14s - loss: 4.7872 - acc: 0.00 - ETA: 14s - loss: 4.7872 - acc: 0.00 - ETA: 14s - loss: 4.7872 - acc: 0.00 - ETA: 14s - loss: 4.7872 - acc: 0.00 - ETA: 14s - loss: 4.7872 - acc: 0.00 - ETA: 14s - loss: 4.7872 - acc: 0.00 - ETA: 14s - loss: 4.7872 - acc: 0.0085\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7155/7155 [==============================] - ETA: 13s - loss: 4.7872 - acc: 0.00 - ETA: 13s - loss: 4.7872 - acc: 0.00 - ETA: 13s - loss: 4.7872 - acc: 0.00 - ETA: 13s - loss: 4.7872 - acc: 0.00 - ETA: 13s - loss: 4.7872 - acc: 0.00 - ETA: 13s - loss: 4.7872 - acc: 0.00 - ETA: 13s - loss: 4.7872 - acc: 0.00 - ETA: 13s - loss: 4.7872 - acc: 0.00 - ETA: 12s - loss: 4.7872 - acc: 0.00 - ETA: 12s - loss: 4.7872 - acc: 0.00 - ETA: 12s - loss: 4.7872 - acc: 0.00 - ETA: 12s - loss: 4.7872 - acc: 0.00 - ETA: 12s - loss: 4.7872 - acc: 0.00 - ETA: 12s - loss: 4.7872 - acc: 0.00 - ETA: 12s - loss: 4.7872 - acc: 0.00 - ETA: 12s - loss: 4.7872 - acc: 0.00 - ETA: 12s - loss: 4.7872 - acc: 0.00 - ETA: 11s - loss: 4.7872 - acc: 0.00 - ETA: 11s - loss: 4.7872 - acc: 0.00 - ETA: 11s - loss: 4.7872 - acc: 0.00 - ETA: 11s - loss: 4.7872 - acc: 0.00 - ETA: 11s - loss: 4.7872 - acc: 0.00 - ETA: 11s - loss: 4.7872 - acc: 0.00 - ETA: 11s - loss: 4.7872 - acc: 0.00 - ETA: 11s - loss: 4.7872 - acc: 0.00 - ETA: 11s - loss: 4.7872 - acc: 0.00 - ETA: 11s - loss: 4.7872 - acc: 0.00 - ETA: 10s - loss: 4.7872 - acc: 0.00 - ETA: 10s - loss: 4.7872 - acc: 0.00 - ETA: 10s - loss: 4.7872 - acc: 0.00 - ETA: 10s - loss: 4.7872 - acc: 0.00 - ETA: 10s - loss: 4.7872 - acc: 0.00 - ETA: 10s - loss: 4.7872 - acc: 0.00 - ETA: 10s - loss: 4.7872 - acc: 0.00 - ETA: 10s - loss: 4.7872 - acc: 0.00 - ETA: 10s - loss: 4.7872 - acc: 0.00 - ETA: 9s - loss: 4.7872 - acc: 0.0080 - ETA: 9s - loss: 4.7872 - acc: 0.008 - ETA: 9s - loss: 4.7872 - acc: 0.008 - ETA: 9s - loss: 4.7872 - acc: 0.008 - ETA: 9s - loss: 4.7872 - acc: 0.008 - ETA: 9s - loss: 4.7872 - acc: 0.008 - ETA: 9s - loss: 4.7872 - acc: 0.008 - ETA: 9s - loss: 4.7872 - acc: 0.008 - ETA: 9s - loss: 4.7872 - acc: 0.008 - ETA: 8s - loss: 4.7872 - acc: 0.008 - ETA: 8s - loss: 4.7872 - acc: 0.008 - ETA: 8s - loss: 4.7872 - acc: 0.008 - ETA: 8s - loss: 4.7872 - acc: 0.008 - ETA: 8s - loss: 4.7872 - acc: 0.008 - ETA: 8s - loss: 4.7872 - acc: 0.008 - ETA: 8s - loss: 4.7872 - acc: 0.008 - ETA: 8s - loss: 4.7872 - acc: 0.008 - ETA: 8s - loss: 4.7872 - acc: 0.008 - ETA: 8s - loss: 4.7872 - acc: 0.008 - ETA: 7s - loss: 4.7872 - acc: 0.008 - ETA: 7s - loss: 4.7872 - acc: 0.008 - ETA: 7s - loss: 4.7872 - acc: 0.008 - ETA: 7s - loss: 4.7872 - acc: 0.008 - ETA: 7s - loss: 4.7872 - acc: 0.008 - ETA: 7s - loss: 4.7872 - acc: 0.008 - ETA: 7s - loss: 4.7872 - acc: 0.008 - ETA: 7s - loss: 4.7872 - acc: 0.008 - ETA: 7s - loss: 4.7872 - acc: 0.008 - ETA: 7s - loss: 4.7872 - acc: 0.008 - ETA: 6s - loss: 4.7872 - acc: 0.008 - ETA: 6s - loss: 4.7872 - acc: 0.008 - ETA: 6s - loss: 4.7872 - acc: 0.008 - ETA: 6s - loss: 4.7872 - acc: 0.008 - ETA: 6s - loss: 4.7872 - acc: 0.008 - ETA: 6s - loss: 4.7872 - acc: 0.007 - ETA: 6s - loss: 4.7872 - acc: 0.008 - ETA: 6s - loss: 4.7872 - acc: 0.008 - ETA: 6s - loss: 4.7872 - acc: 0.008 - ETA: 6s - loss: 4.7872 - acc: 0.008 - ETA: 6s - loss: 4.7872 - acc: 0.008 - ETA: 5s - loss: 4.7872 - acc: 0.008 - ETA: 5s - loss: 4.7872 - acc: 0.008 - ETA: 5s - loss: 4.7872 - acc: 0.008 - ETA: 5s - loss: 4.7872 - acc: 0.008 - ETA: 5s - loss: 4.7872 - acc: 0.008 - ETA: 5s - loss: 4.7872 - acc: 0.008 - ETA: 5s - loss: 4.7872 - acc: 0.007 - ETA: 5s - loss: 4.7872 - acc: 0.007 - ETA: 5s - loss: 4.7872 - acc: 0.007 - ETA: 5s - loss: 4.7872 - acc: 0.007 - ETA: 5s - loss: 4.7872 - acc: 0.007 - ETA: 4s - loss: 4.7872 - acc: 0.007 - ETA: 4s - loss: 4.7872 - acc: 0.007 - ETA: 4s - loss: 4.7872 - acc: 0.007 - ETA: 4s - loss: 4.7872 - acc: 0.007 - ETA: 4s - loss: 4.7872 - acc: 0.007 - ETA: 4s - loss: 4.7872 - acc: 0.007 - ETA: 4s - loss: 4.7872 - acc: 0.007 - ETA: 4s - loss: 4.7872 - acc: 0.007 - ETA: 4s - loss: 4.7872 - acc: 0.007 - ETA: 4s - loss: 4.7872 - acc: 0.007 - ETA: 4s - loss: 4.7872 - acc: 0.007 - ETA: 3s - loss: 4.7872 - acc: 0.007 - ETA: 3s - loss: 4.7872 - acc: 0.007 - ETA: 3s - loss: 4.7872 - acc: 0.007 - ETA: 3s - loss: 4.7873 - acc: 0.007 - ETA: 3s - loss: 4.7873 - acc: 0.007 - ETA: 3s - loss: 4.7873 - acc: 0.007 - ETA: 3s - loss: 4.7873 - acc: 0.007 - ETA: 3s - loss: 4.7873 - acc: 0.007 - ETA: 3s - loss: 4.7873 - acc: 0.007 - ETA: 3s - loss: 4.7873 - acc: 0.007 - ETA: 3s - loss: 4.7872 - acc: 0.007 - ETA: 3s - loss: 4.7873 - acc: 0.007 - ETA: 2s - loss: 4.7873 - acc: 0.007 - ETA: 2s - loss: 4.7872 - acc: 0.007 - ETA: 2s - loss: 4.7872 - acc: 0.007 - ETA: 2s - loss: 4.7872 - acc: 0.007 - ETA: 2s - loss: 4.7873 - acc: 0.007 - ETA: 2s - loss: 4.7873 - acc: 0.007 - ETA: 2s - loss: 4.7873 - acc: 0.007 - ETA: 2s - loss: 4.7873 - acc: 0.007 - ETA: 2s - loss: 4.7873 - acc: 0.007 - ETA: 2s - loss: 4.7873 - acc: 0.007 - ETA: 2s - loss: 4.7872 - acc: 0.007 - ETA: 1s - loss: 4.7873 - acc: 0.007 - ETA: 1s - loss: 4.7873 - acc: 0.007 - ETA: 1s - loss: 4.7873 - acc: 0.007 - ETA: 1s - loss: 4.7873 - acc: 0.007 - ETA: 1s - loss: 4.7873 - acc: 0.007 - ETA: 1s - loss: 4.7873 - acc: 0.007 - ETA: 1s - loss: 4.7873 - acc: 0.007 - ETA: 1s - loss: 4.7873 - acc: 0.007 - ETA: 1s - loss: 4.7873 - acc: 0.007 - ETA: 1s - loss: 4.7873 - acc: 0.007 - ETA: 1s - loss: 4.7872 - acc: 0.007 - ETA: 1s - loss: 4.7872 - acc: 0.007 - ETA: 0s - loss: 4.7872 - acc: 0.007 - ETA: 0s - loss: 4.7872 - acc: 0.007 - ETA: 0s - loss: 4.7872 - acc: 0.007 - ETA: 0s - loss: 4.7873 - acc: 0.007 - ETA: 0s - loss: 4.7873 - acc: 0.007 - ETA: 0s - loss: 4.7873 - acc: 0.007 - ETA: 0s - loss: 4.7873 - acc: 0.007 - ETA: 0s - loss: 4.7873 - acc: 0.007 - ETA: 0s - loss: 4.7873 - acc: 0.007 - ETA: 0s - loss: 4.7873 - acc: 0.007 - ETA: 0s - loss: 4.7873 - acc: 0.007 - 38s 5ms/step - loss: 4.7873 - acc: 0.0075 - val_loss: 4.7866 - val_acc: 0.0104\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a51da85dd8>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, Y_train, epochs=1, validation_data=(X_valid, Y_valid), \n",
    "          verbose=1, batch_size = 10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10357/10357 [==============================] - ETA: 1: - ETA: 1: - ETA: 55s - ETA: 47 - ETA: 42 - ETA: 39 - ETA: 36 - ETA: 35 - ETA: 33 - ETA: 32 - ETA: 31 - ETA: 30 - ETA: 30 - ETA: 29 - ETA: 29 - ETA: 28 - ETA: 28 - ETA: 28 - ETA: 27 - ETA: 27 - ETA: 27 - ETA: 26 - ETA: 26 - ETA: 26 - ETA: 26 - ETA: 25 - ETA: 25 - ETA: 25 - ETA: 25 - ETA: 25 - ETA: 24 - ETA: 24 - ETA: 24 - ETA: 24 - ETA: 24 - ETA: 24 - ETA: 23 - ETA: 23 - ETA: 23 - ETA: 23 - ETA: 23 - ETA: 23 - ETA: 23 - ETA: 22 - ETA: 22 - ETA: 22 - ETA: 22 - ETA: 22 - ETA: 22 - ETA: 22 - ETA: 22 - ETA: 22 - ETA: 21 - ETA: 21 - ETA: 21 - ETA: 21 - ETA: 21 - ETA: 21 - ETA: 21 - ETA: 21 - ETA: 21 - ETA: 21 - ETA: 20 - ETA: 20 - ETA: 20 - ETA: 20 - ETA: 20 - ETA: 20 - ETA: 20 - ETA: 20 - ETA: 20 - ETA: 20 - ETA: 19 - ETA: 19 - ETA: 19 - ETA: 19 - ETA: 19 - ETA: 19 - ETA: 19 - ETA: 19 - ETA: 19 - ETA: 19 - ETA: 19 - ETA: 18 - ETA: 18 - ETA: 18 - ETA: 18 - ETA: 18 - ETA: 18 - ETA: 18 - ETA: 18 - ETA: 18 - ETA: 18 - ETA: 18 - ETA: 17 - ETA: 17 - ETA: 17 - ETA: 17 - ETA: 17 - ETA: 17 - ETA: 17 - ETA: 17 - ETA: 17 - ETA: 17 - ETA: 17 - ETA: 17 - ETA: 16 - ETA: 16 - ETA: 16 - ETA: 16 - ETA: 16 - ETA: 16 - ETA: 16 - ETA: 16 - ETA: 16 - ETA: 16 - ETA: 16 - ETA: 16 - ETA: 15 - ETA: 15 - ETA: 15 - ETA: 15 - ETA: 15 - ETA: 15 - ETA: 15 - ETA: 15 - ETA: 15 - ETA: 15 - ETA: 15 - ETA: 15 - ETA: 14 - ETA: 14 - ETA: 14 - ETA: 14 - ETA: 14 - ETA: 14 - ETA: 14 - ETA: 14 - ETA: 14 - ETA: 14 - ETA: 14 - ETA: 14 - ETA: 14 - ETA: 13 - ETA: 13 - ETA: 13 - ETA: 13 - ETA: 13 - ETA: 13 - ETA: 13 - ETA: 13 - ETA: 13 - ETA: 13 - ETA: 13 - ETA: 13 - ETA: 12 - ETA: 12 - ETA: 12 - ETA: 12 - ETA: 12 - ETA: 12 - ETA: 12 - ETA: 12 - ETA: 12 - ETA: 12 - ETA: 12 - ETA: 12 - ETA: 12 - ETA: 11 - ETA: 11 - ETA: 11 - ETA: 11 - ETA: 11 - ETA: 11 - ETA: 11 - ETA: 11 - ETA: 11 - ETA: 11 - ETA: 11 - ETA: 11 - ETA: 11 - ETA: 10 - ETA: 10 - ETA: 10 - ETA: 10 - ETA: 10 - ETA: 10 - ETA: 10 - ETA: 10 - ETA: 10 - ETA: 10 - ETA: 10 - ETA: 10 - ETA: 9 - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 25s 2ms/step\n"
     ]
    }
   ],
   "source": [
    "preds = model.predict(x_test, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.00835556,  0.00853121,  0.00836687,  0.00851014,  0.00827073,\n",
       "        0.0082788 ,  0.00837732,  0.00847739,  0.0083684 ,  0.00847653,\n",
       "        0.00832976,  0.008464  ,  0.00826276,  0.00841523,  0.00835403,\n",
       "        0.00834495,  0.00827578,  0.00837536,  0.0082967 ,  0.00836602,\n",
       "        0.00839202,  0.00830929,  0.00823732,  0.00820819,  0.00821757,\n",
       "        0.00830929,  0.00842428,  0.00823661,  0.00834666,  0.00824574,\n",
       "        0.00839891,  0.00833957,  0.00828837,  0.0084256 ,  0.00823529,\n",
       "        0.00836075,  0.00830028,  0.00833076,  0.00825199,  0.0083774 ,\n",
       "        0.00835908,  0.0083459 ,  0.00842251,  0.00827303,  0.00825865,\n",
       "        0.00831328,  0.00824012,  0.00823021,  0.00826993,  0.0082363 ,\n",
       "        0.00830675,  0.00827994,  0.00848503,  0.00827832,  0.00823001,\n",
       "        0.00834625,  0.00832453,  0.00832008,  0.00833294,  0.00841315,\n",
       "        0.0083438 ,  0.00842397,  0.00827949,  0.00832033,  0.00832474,\n",
       "        0.00821096,  0.00825325,  0.00834115,  0.00839081,  0.00846793,\n",
       "        0.00832096,  0.00828997,  0.00828052,  0.00847111,  0.0083208 ,\n",
       "        0.00840943,  0.0082876 ,  0.00832884,  0.00833667,  0.00827476,\n",
       "        0.00843817,  0.00825195,  0.00831143,  0.00820983,  0.0083204 ,\n",
       "        0.00826184,  0.00832869,  0.00845323,  0.00838768,  0.00827827,\n",
       "        0.00836711,  0.00819744,  0.00829143,  0.00840147,  0.00845773,\n",
       "        0.00837355,  0.00831681,  0.00850961,  0.00836327,  0.00829554,\n",
       "        0.00849438,  0.00840441,  0.00838203,  0.00826658,  0.00826753,\n",
       "        0.00829424,  0.0082737 ,  0.00831762,  0.00822936,  0.00847616,\n",
       "        0.0083558 ,  0.00834884,  0.00826436,  0.00828445,  0.00833722,\n",
       "        0.00825679,  0.00831935,  0.00835589,  0.00826205,  0.00831734], dtype=float32)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sub = pd.DataFrame(preds)\n",
    "# Set column names to those generated by the one-hot encoding earlier\n",
    "col_names = one_hot.columns.values\n",
    "sub.columns = col_names\n",
    "# Insert the column id from the sample_submission at the start of the data frame\n",
    "sub.insert(0, 'id', df_test['id'])\n",
    "sub.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
